<project><prname> CEBD 1260 –Introduction to machine learning</prname>
<idnum>5819</idnum>
<terminologyset>cegep</terminologyset>
<workflow><wfname>StVrsn_CEBD 1260 –Introduction to machine learning --</wfname>
<wfauthor>Instructor: Yimin Nie</wfauthor>
<wfdescription>&lt;p&gt;This course is the introduction todata analysis and machine learning (ML) algorithms/models/methodologies. Based on the applications in machine learning /AI in multiple businesses (i.e. finance, user behavior analysis, retailer services, etc.), the detailed models using ML to solve these practical questions will be presented and the implementation of the code will be guided step by step. &lt;/p&gt;&lt;p&gt;The following topics related to general machine learning techniques will be covered: common-used data analysis/machine learning programming tools, exploratory data analysis, feature engineering, supervised/unsupervised learning, regression, classification, tree-based models, boosting and ensemble methods, time series, data visualization, clustering, dimensionality reduction and their applications in various business cases.&lt;/p&gt;</wfdescription>
<wfid>1</wfid>
<wftype>course</wftype>
<usedwfARRAY></usedwfARRAY>
<tagsetARRAY></tagsetARRAY>
<wfdata><column><columnname>HW</columnname>
<columntext>Preparation (students)</columntext>
<columnnodetext>Preparation (students)</columnnodetext>
<columnoutcomevisibility>undefined</columnoutcomevisibility>
</column>
<column><columnname>CUS1</columnname>
<columnimage>solo</columnimage>
<columncolour>#a3b9df</columncolour>
<columntext>Tutorial (Korbit)</columntext>
<columnnodetext>Tutorial (Korbit)</columnnodetext>
<columnoutcomevisibility>undefined</columnoutcomevisibility>
</column>
<column><columnname>AC</columnname>
<columntext>Lesson / Lecture</columntext>
<columnnodetext>Lesson / Lecture</columnnodetext>
<columnoutcomevisibility>true</columnoutcomevisibility>
</column>
<column><columnname>CUS3</columnname>
<columnimage>gears</columnimage>
<columncolour>#6738ff</columncolour>
<columntext>In Class Practice</columntext>
<columnnodetext>In Class Practice</columnnodetext>
<columnoutcomevisibility>undefined</columnoutcomevisibility>
</column>
<column><columnname>CUS2</columnname>
<columnimage>general</columnimage>
<columncolour>#116444</columncolour>
<columntext>Offline Assignments</columntext>
<columnnodetext>Offline Assignments</columnnodetext>
<columnoutcomevisibility>undefined</columnoutcomevisibility>
</column>
<column><columnname>SA</columnname>
<columntext>Project / Assessments</columntext>
<columnnodetext>Project / Assessments</columnnodetext>
</column>
<week><weekid>278</weekid>
<weekname>Week 1: general terms and development review for ML</weekname>
<node><name>Pre-requisite knowledge in ML and AI</name>
<id>3950</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>HW</column>
<textHTML>&lt;ul&gt;&lt;li&gt;important concepts and terminologies in ML/AI&lt;/li&gt;&lt;li&gt;Data mining, machine learning and AI definition and their relationship/differences&lt;/li&gt;&lt;li&gt;often-used platform and toolbox in ML/AI&lt;/li&gt;&lt;li&gt;Role /work flow as data analyst/scientist/engineer&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>163.60000610351562</textheight>
<isdropped>true</isdropped>
<link><targetid>3951</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Session #1: Introduction to Development Tools</name>
<id>3951</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;ul&gt;&lt;li&gt;What is machine learning (ML) and artificial intelligence (AI)&lt;/li&gt;&lt;li&gt;What are their relationships and differences&lt;/li&gt;&lt;li&gt;Over view of often-used toolkits and programming languages in ML/AI development&lt;/li&gt;&lt;li&gt;Career path and learning path of being a data scientist and data engineer&lt;/li&gt;&lt;li&gt;Overview of applications in diverse businesses for ML/AI&lt;/li&gt;&lt;li&gt;Overview and guideline for ML libs installation and environment setting&lt;/li&gt;&lt;li&gt;Assign multiple projects for students according to their timeline and difficulties of the project&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>264</textheight>
<timevalue>180</timevalue>
<timeunit>min</timeunit>
<link><targetid>12</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
<link><targetid>5698</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Determine Groups &amp; Projects</name>
<id>5698</id>
<lefticon>exam</lefticon>
<righticon>undefined</righticon>
<column>SA</column>
<textHTML>&lt;ul&gt;&lt;li&gt;&lt;em&gt;Group are assigned in class (Sess. #2 )&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;Projects are chosen in class (Sess. #2)&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;em&gt;﻿&lt;/em&gt;&lt;/p&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>187</textheight>
</node>
<node><name>Introduction to Korbit</name>
<id>3952</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>undefined</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>10</textheight>
<timevalue>10</timevalue>
<timeunit>min</timeunit>
</node>
<node><name>Module 1 (M1): Linear Algebra Basics</name>
<id>279</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS1</column>
<textHTML>&lt;p&gt;To support your learning, the following sub-modules may be helpful to review after session #1. These are optional:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Why Learn Linear Algebra?&lt;/li&gt;&lt;li&gt;Vectors and Matrices and Scalars&lt;/li&gt;&lt;li&gt;Addition and Scalar Multiplication and Transpose&lt;/li&gt;&lt;li&gt;Dot Product&lt;/li&gt;&lt;li&gt;Norm and Euclidean Distance&lt;/li&gt;&lt;li&gt;Matrix Multiplication&lt;/li&gt;&lt;li&gt;Identity Matrix&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>178</textheight>
<link><targetid>12</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>M2: Probability Basics	</name>
<id>280</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS1</column>
<textHTML>&lt;p&gt;To support your learning, the following sub-modules may be helpful to review after session #1. These are optional:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Introduction to Probability&lt;/li&gt;&lt;li&gt;Discrete vs. Continuous Random Variables&lt;/li&gt;&lt;li&gt;Expected Value vs. Sample Mean&lt;/li&gt;&lt;li&gt;Variance and Standard Deviation&lt;/li&gt;&lt;li&gt;Binomial Distribution&lt;/li&gt;&lt;li&gt;Normal distribution&lt;/li&gt;&lt;li&gt;conditional probability&lt;/li&gt;&lt;li&gt;Joint and marginal probability&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>206</textheight>
<link><targetid>12</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
</week>
<week><weekid>2</weekid>
<weekname>Week 2: general methodologies in ML</weekname>
<node><name>M5: Machine Learning Overview</name>
<id>4207</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS1</column>
<textHTML>&lt;p&gt;To master session #2, review Korbit sub-modules before class:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Motivation of ML&lt;/li&gt;&lt;li&gt;Supervised learning&lt;/li&gt;&lt;li&gt;Overfitting and underfitting&lt;/li&gt;&lt;li&gt;Overview of cross validation (HD)&lt;/li&gt;&lt;li&gt;Bias-variance trade-off and decomposition&lt;/li&gt;&lt;li&gt;Gradient descent&lt;/li&gt;&lt;li&gt;Summary of ML&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>178</textheight>
<link><targetid>12</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Readings on ML methodologies</name>
<id>3953</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>HW</column>
<textHTML>&lt;p&gt;Complete readings:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Types of learning &lt;/strong&gt;(supervised &amp;amp; unsupervised)&lt;/li&gt;&lt;li&gt;overfitting and underfitting&lt;/li&gt;&lt;li&gt;Generalization and robustness in ML&lt;/li&gt;&lt;li&gt;Overfit and underfit concept&lt;/li&gt;&lt;li&gt;Identify data format(input features and target)&lt;/li&gt;&lt;li&gt;How to prepare data (split dataset)&lt;/li&gt;&lt;li&gt;Train, validation and test and how they work&lt;/li&gt;&lt;li&gt;Concept of loss function&lt;/li&gt;&lt;li&gt;Cross validation strategy&lt;/li&gt;&lt;li&gt;What are evaluation metrics used in ML&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>250</textheight>
<link><targetid>12</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Session #2: General methodologies</name>
<id>12</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;Provide a comprehensive bigger picture and methodologies used in ML and key techniques&lt;/p&gt;&lt;ul&gt;&lt;li&gt;generalization of ML&lt;/li&gt;&lt;li&gt;overfitting and underfitting&lt;/li&gt;&lt;li&gt;learning types and format&lt;/li&gt;&lt;li&gt;split data into train, valid and test&lt;/li&gt;&lt;li&gt;Categorical and numerical features&lt;/li&gt;&lt;li&gt;loss function as objective&lt;/li&gt;&lt;li&gt;why does ML use loss&lt;/li&gt;&lt;li&gt;loss functions for classification and regression (just overview and concept build)&lt;/li&gt;&lt;li&gt;how to learn(gradient descent)&lt;/li&gt;&lt;li&gt;visualize how gradient decent works&lt;/li&gt;&lt;li&gt;Cross validation strategy&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>250</textheight>
<timevalue>110</timevalue>
<timeunit>min</timeunit>
<link><targetid>3955</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>2: Code Demo &amp; Practice</name>
<id>3955</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;ul&gt;&lt;li&gt;Read/load data, split data by input features and target variables using pandas&lt;/li&gt;&lt;li&gt;Identify categorical and numerical features in the dataset&lt;/li&gt;&lt;li&gt;Split data into train, validation and test by sklearn API&lt;/li&gt;&lt;li&gt;Split total data by cross validation strategy using sklearn API&lt;/li&gt;&lt;li&gt;Get to know loss functions provided by sklearn API&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>192</textheight>
<timevalue>70</timevalue>
<timeunit>min</timeunit>
<link><targetid>3956</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
<link><targetid>10</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
<link><targetid>3957</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>#2 Theoretical assignment</name>
<id>3956</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS2</column>
<textHTML>&lt;p&gt;Provided theoretical assignment in order to&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Understand supervised/unsupervised learning tasks and classification/regression&lt;/li&gt;&lt;li&gt;data splitting strategies&lt;/li&gt;&lt;li&gt;imbalanced dataset (extensive reading and review)&lt;/li&gt;&lt;li&gt;learning rule calculation for linear regression(bonus point)&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>163</textheight>
</node>
<node><name>#2 Coding assignment</name>
<id>3957</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>SA</column>
<textHTML>&lt;p&gt;For projects, students complete the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;practice data splitting and sklearn API&lt;/li&gt;&lt;li&gt;practice cross validation splitting and sklearn API&lt;/li&gt;&lt;li&gt;practice calculate loss using sklearn API&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Notes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;em&gt;Group are assigned in class (Sess. #2 )&lt;/em&gt;&lt;/li&gt;&lt;li&gt;&lt;em&gt;Projects are chosen in class (Sess. #2)&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;em&gt;﻿&lt;/em&gt;&lt;/p&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>187</textheight>
</node>
</week>
<week><weekid>4</weekid>
<weekname>Week 3: Data preprocessing : data IO operation</weekname>
<node><name>Readings</name>
<id>146</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>HW</column>
<textHTML>&lt;p&gt;Provide some reading lists for understanding:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Categorical and numerical variables&lt;/li&gt;&lt;li&gt;refresh of pandas data processing API&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>76</textheight>
<link><targetid>10</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Session #3 Data Preprocessing 1 </name>
<id>10</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;&lt;strong&gt;Data Preprocessing 1: Basic Processing&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Lecture includes the following concepts &amp;amp; topics:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Missing data analysis&lt;/li&gt;&lt;li&gt;Data type check and conversion&lt;/li&gt;&lt;li&gt;Indexing and query data by conditions&lt;/li&gt;&lt;li&gt;Groupby and merge tables&lt;/li&gt;&lt;li&gt;Lambda expressions in pandas&lt;/li&gt;&lt;li&gt;Process categorical data (label encoding and one hot encoding)&lt;/li&gt;&lt;li&gt;Process numerical data(normalize)&lt;/li&gt;&lt;li&gt;Sparse matrix for large dimensional features&lt;/li&gt;&lt;li&gt;Memory management skills for larger data&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>255</textheight>
<timevalue>100</timevalue>
<timeunit>min</timeunit>
<link><targetid>4078</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
<link><targetid>23</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>3: Code Demo &amp; Practice</name>
<id>4078</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;&lt;strong&gt;Data preprocessing&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We provided coding practice for preprocessing in pandas using student's project datasets as demo&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Data I/O in pandas&lt;/li&gt;&lt;li&gt;Data type conversion&lt;/li&gt;&lt;li&gt;Find and fill missing data&lt;/li&gt;&lt;li&gt;Indexing subset from total data&lt;/li&gt;&lt;li&gt;Table aggregation, join and merge&lt;/li&gt;&lt;li&gt;Fast processing using lambda expression&lt;/li&gt;&lt;li&gt;Label &amp;amp; one-hot encoding categorical data&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>226</textheight>
<timevalue>90</timevalue>
<timeunit>min</timeunit>
<link><targetid>4079</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
<link><targetid>2425</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>M3: Data Preprocessing </name>
<id>2425</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS1</column>
<textHTML>&lt;p&gt;Review Korbit sub-modules after session #3:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;What is a Dataset?&lt;/li&gt;&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;&lt;li&gt;Overview of Data Preprocessing&lt;/li&gt;&lt;li&gt;One-Hot Encoding&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>106</textheight>
<link><targetid>23</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>#3 Assignment for basic processing</name>
<id>4079</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>SA</column>
<textHTML>&lt;p&gt;Asked students to practice all aspects of basic data preprocessing based on their own project datasets&lt;/p&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>53.20000457763672</textheight>
<isdropped>true</isdropped>
</node>
</week>
<week><weekid>5</weekid>
<weekname>Week 4: data processing: feature engineering</weekname>
<node><name>M4: Data Preprocessing II</name>
<id>2437</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS1</column>
<textHTML>&lt;p&gt;Review Korbit sub-modules before class:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Feature Engineering&lt;/li&gt;&lt;li&gt;feature selection&lt;/li&gt;&lt;li&gt;feature scaling&lt;/li&gt;&lt;li&gt;feature importance&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>106</textheight>
<link><targetid>23</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Session #4: Data Preprocessing 2</name>
<id>23</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;&lt;strong&gt;Data Preprocessing 2: Feature Engineering&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Lecture on feature engineering, including the following concepts and processes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Feature engineering for timestamp/date related features&lt;/li&gt;&lt;li&gt;Feature engineering for categorical features&lt;/li&gt;&lt;li&gt;Feature engineering at aggregation level stats&lt;/li&gt;&lt;li&gt;Feature engineering on business sense&lt;/li&gt;&lt;li&gt;Feature selection skills&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>174</textheight>
<timevalue>100</timevalue>
<timeunit>min</timeunit>
<link><targetid>139</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>4: Code Demo &amp; Practice</name>
<id>139</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;&lt;strong&gt;&lt;span class="ql-cursor"&gt;﻿&lt;/span&gt;Feature engineering code &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In-class code practice on feature engineering includes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Process timestamp, extract time information&lt;/li&gt;&lt;li&gt;Extract features on categorical data&lt;/li&gt;&lt;li&gt;feature aggregation&lt;/li&gt;&lt;li&gt;Customize features based on business understanding (open practice)&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>168</textheight>
<timevalue>80</timevalue>
<timeunit>min</timeunit>
<link><targetid>101</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
<link><targetid>148</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>#4 Assignment: feature engineering</name>
<id>148</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>SA</column>
<textHTML>&lt;p&gt;&lt;strong&gt;Assignment for feature engineering&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Replay in-class code demo by using your own dataset&lt;/li&gt;&lt;li&gt;Come up with new business-orientated features based on your own project dataset&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>118</textheight>
</node>
</week>
<week><weekid>6</weekid>
<weekname>Week 5: Supervised learning (I): classification models</weekname>
<node><name>M4: Machine Learning Models </name>
<id>4208</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS1</column>
<textHTML>&lt;p&gt;Review Korbit sub-modules before class: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;Logistic regression basic&lt;/li&gt;&lt;li&gt;Sigmoid function&lt;/li&gt;&lt;li&gt;Binary classification and imbalanced classes&lt;/li&gt;&lt;li&gt;binary classification&lt;/li&gt;&lt;li&gt;Evaluation metrics for classification&lt;/li&gt;&lt;li&gt;Parameters and hyper-parameters&lt;/li&gt;&lt;li&gt;Hyperparameter tuning&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>163</textheight>
<link><targetid>101</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Readings</name>
<id>4080</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>HW</column>
<textHTML>&lt;ul&gt;&lt;li&gt;Tree based models&lt;/li&gt;&lt;li&gt;Entropy and information gain&lt;/li&gt;&lt;li&gt;Precision and recall, F1 score&lt;/li&gt;&lt;li&gt;ROC-AUC score&lt;/li&gt;&lt;li&gt;Random Forest &lt;/li&gt;&lt;li&gt;Gradient boosting tree classifier&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>120</textheight>
<link><targetid>101</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Session #5: Classification models</name>
<id>101</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;Lecture on classification models, including the following concepts and processes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Linear classifier ; perceptron and logistic regression&lt;/li&gt;&lt;li&gt;Log loss for binary classification&lt;/li&gt;&lt;li&gt;Softmax for multi-class classification &lt;/li&gt;&lt;li&gt;Sigmoid activation function for output&lt;/li&gt;&lt;li&gt;Decision tree and its principals&lt;/li&gt;&lt;li&gt;Entropy and information gain&lt;/li&gt;&lt;li&gt;Shannon information and its general property&lt;/li&gt;&lt;li&gt;Random forest &lt;/li&gt;&lt;li&gt;Gradient boosting machine&lt;/li&gt;&lt;li&gt;Evaluation metrics for classification&lt;/li&gt;&lt;li class="ql-indent-1"&gt;F1 score, confusion matrix&lt;/li&gt;&lt;li class="ql-indent-1"&gt;ROC curve&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>258</textheight>
<timevalue>130</timevalue>
<timeunit>min</timeunit>
<link><targetid>140</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>5: Code Demo &amp; Practice</name>
<id>140</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;&lt;strong&gt;5: Train first model by classifier&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Build first model pipeline using linear classifier (perceptron) and nonlinear classifier (random forest and gradient boosting machine)&lt;/p&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>52</textheight>
<timevalue>50</timevalue>
<timeunit>min</timeunit>
<link><targetid>4081</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
<link><targetid>121</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
<link><targetid>150</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>#5 assignment : theory</name>
<id>150</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS2</column>
<textHTML>&lt;p&gt;&lt;strong&gt;#5 theory assignment includes&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;understand entropy and its property&lt;/li&gt;&lt;li&gt;derive learning rule for logistic regression based on binary log-loss&lt;/li&gt;&lt;li&gt;understanding F1 score&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>106</textheight>
</node>
<node><name>#5 code assignment</name>
<id>4081</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>SA</column>
<textHTML>&lt;p&gt;Train your base-line model by random forest and perceptron&lt;/p&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>38</textheight>
</node>
</week>
<week><weekid>21</weekid>
<weekname>Week 6: supervised learning (II): regression, regularization and ensemble methods</weekname>
<node><name>M4: Regression and regularization</name>
<id>4209</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS1</column>
<textHTML>&lt;p&gt;Rreview Korbit sub-modules before class:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Linear regression&lt;/li&gt;&lt;li&gt;Hyperparameters tuning&lt;/li&gt;&lt;li&gt;&lt;em&gt;Regularization (this one isn't a sub-module)&lt;/em&gt;&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>106</textheight>
<link><targetid>121</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Readings</name>
<id>4337</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>HW</column>
<textHTML>&lt;ul&gt;&lt;li&gt;Regularization and  penalty in ML&lt;/li&gt;&lt;li&gt;L1 and L2 mathematical principals&lt;/li&gt;&lt;li&gt;Ensemble methods&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>76</textheight>
<link><targetid>121</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Session #6: Regression and regularization</name>
<id>121</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;Lecture on Regression, including the following concepts:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Linear regression&lt;/li&gt;&lt;li&gt;Regularization (L1 and L2)&lt;/li&gt;&lt;li&gt;Ensemble: bagging and boosting&lt;/li&gt;&lt;li&gt;LightGBM and Xgboost introduction&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>118</textheight>
<timevalue>100</timevalue>
<timeunit>min</timeunit>
<link><targetid>141</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>6: Code Demo &amp; Practice</name>
<id>141</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;ul&gt;&lt;li&gt;Train lightGBM&lt;/li&gt;&lt;li&gt;Hyper-parameters tuning in lightGBM&lt;/li&gt;&lt;li&gt;Learning rate, early-stopping tuning&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>76</textheight>
<timevalue>80</timevalue>
<timeunit>min</timeunit>
<link><targetid>124</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
<link><targetid>153</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>#6 assignment</name>
<id>153</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS2</column>
<textHTML>&lt;p&gt;&lt;strong&gt;Project coding practice/ activity&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;train models using lightGBM and hyperparameter tuning, early stopping&lt;/p&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>77</textheight>
</node>
</week>
<week><weekid>115</weekid>
<weekname>Week 7: advanced feature engineering, Bayesian model, docker image
</weekname>
<node><name>Install docker image toolbox</name>
<id>4338</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>HW</column>
<textHTML>&lt;ul&gt;&lt;li&gt;review and install docker image box&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>62</textheight>
<link><targetid>124</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Session #7:  Advanced feature engineering</name>
<id>124</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;Lecture on advanced feature engineering and docker images includes&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Feature importance and rank&lt;/li&gt;&lt;li&gt;Feature selection skills&lt;/li&gt;&lt;li&gt;Target encoding skill&lt;/li&gt;&lt;li&gt;Bayesian models in probability perspective&lt;/li&gt;&lt;li&gt;Spam mail detection overview by naive Bayes&lt;/li&gt;&lt;li&gt;Docker image: shift your code entirely to clients&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>192</textheight>
<timevalue>90</timevalue>
<timeunit>min</timeunit>
<link><targetid>142</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
<link><targetid>127</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>7: Code Demo &amp; Practice</name>
<id>142</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;ul&gt;&lt;li&gt;Target encoding&lt;/li&gt;&lt;li&gt;Other advanced feature engineering skills&lt;/li&gt;&lt;li&gt;create docker images step by step&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>91</textheight>
<timevalue>90</timevalue>
<timeunit>min</timeunit>
<link><targetid>156</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>#7 assignment</name>
<id>156</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS2</column>
<textHTML>&lt;ul&gt;&lt;li&gt;Apply target encoding on project data&lt;/li&gt;&lt;li&gt;Fine tune pipeline model&lt;/li&gt;&lt;li&gt;Use feature selection and rank to improve model performance&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>90</textheight>
</node>
</week>
<week><weekid>119</weekid>
<weekname>Week 8: time series analysis
</weekname>
<node><name>Session #8: Time series analysis</name>
<id>127</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;Lecture on time series analysis, including the following &lt;/p&gt;&lt;ul&gt;&lt;li&gt;Basic property of time series data&lt;/li&gt;&lt;li&gt;Some important index in time series data&lt;/li&gt;&lt;li&gt;Strategy to split time series data&lt;/li&gt;&lt;li&gt;Moving averge&lt;/li&gt;&lt;li&gt;Culmulative sum&lt;/li&gt;&lt;li&gt;Lagged features&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>160</textheight>
<timevalue>120</timevalue>
<timeunit>min</timeunit>
<link><targetid>143</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>8: Code Demo &amp; Practice</name>
<id>143</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;In-class coding demo for time series provided&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Pipeline for time series data analysis&lt;/li&gt;&lt;li&gt;Important features demo to improve time series model &lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>90</textheight>
<timevalue>60</timevalue>
<timeunit>min</timeunit>
<link><targetid>133</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
<link><targetid>159</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>#8 assignment</name>
<id>159</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS2</column>
<textHTML>&lt;ul&gt;&lt;li&gt;continue fine tune models&lt;/li&gt;&lt;li&gt;practice rolling mean&lt;/li&gt;&lt;li&gt;practice cumsum &lt;/li&gt;&lt;li&gt;practice lagged features for time series demo data&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>104</textheight>
</node>
</week>
<week><weekid>117</weekid>
<weekname>Week 9: unsupervised learning, K-mean clustering, dimensionality reduction
</weekname>
<node><name>Session #9: K-means clustering and dimensionality reduction</name>
<id>133</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;Lecture on K-means clustering &amp;amp; Dimensionality Reduction, including the following concepts and topics:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Dimensionality reduction introduction&lt;/li&gt;&lt;li&gt;PCA analysis introduction&lt;/li&gt;&lt;li&gt;Clustering: K-means&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>118</textheight>
<timevalue>90</timevalue>
<timeunit>min</timeunit>
<link><targetid>145</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>9: Code Demo &amp; Practice</name>
<id>145</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;ul&gt;&lt;li&gt;Demo for clustering&lt;/li&gt;&lt;li&gt;Demo for PCA using images&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>62</textheight>
<timevalue>90</timevalue>
<timeunit>min</timeunit>
<link><targetid>147</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
<link><targetid>164</targetid>
<portstyle>sourcePort=OUTe;targetPort=INn;</portstyle>
</link>
</node>
<node><name>#9 assignment</name>
<id>164</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS2</column>
<textHTML>&lt;p&gt;&lt;strong&gt;Coding practice. &lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;replay k-means clustering and PCA from demo code&lt;/li&gt;&lt;li&gt;work on project code and push them into production level code base&lt;/li&gt;&lt;li&gt;review ML coding skills and be ready for final test&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>132</textheight>
</node>
<node><name>M9: K-means, dimensionality reduction</name>
<id>4342</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>CUS1</column>
<textHTML>&lt;ul&gt;&lt;li&gt;clustering k-means&lt;/li&gt;&lt;li&gt;dimensionality reduction&lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>62.80000305175781</textheight>
<isdropped>true</isdropped>
</node>
</week>
<week><weekid>116</weekid>
<weekname>Week 10 - Final test</weekname>
<node><name>Finalize Project Components</name>
<id>147</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>HW</column>
<textHTML>&lt;p&gt;The final project should contain &lt;/p&gt;&lt;ol&gt;&lt;li&gt;The complete and clean code developed to implement the analyses &lt;/li&gt;&lt;li&gt;A README file briefly summarizing the results obtained and explaining how to reproduce the results&lt;/li&gt;&lt;li&gt;A report of project describing the analysis and findings&lt;/li&gt;&lt;/ol&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>146</textheight>
<link><targetid>136</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Comprehensive in-class final test</name>
<id>136</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;p&gt;Use selected small data set for final in-class evaluation/test and competition&lt;/p&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>38</textheight>
<timevalue>150</timevalue>
<timeunit>min</timeunit>
<link><targetid>4339</targetid>
<portstyle>sourcePort=OUTs;targetPort=INn;</portstyle>
</link>
</node>
<node><name>Summary of the course</name>
<id>4339</id>
<lefticon>undefined</lefticon>
<righticon>undefined</righticon>
<column>AC</column>
<textHTML>&lt;ul&gt;&lt;li&gt;summary the course&lt;/li&gt;&lt;li&gt;further readings/learnings &lt;/li&gt;&lt;/ul&gt;</textHTML>
<linkedwf>undefined</linkedwf>
<textheight>62</textheight>
<timevalue>30</timevalue>
<timeunit>min</timeunit>
</node>
</week>
<comment><textHTML>&lt;p&gt;Is this pre-requisite material helpful or recommended? Of the topics here and in Korbit, which should students review?&lt;/p&gt;&lt;p&gt;Helpful suggestion: use bold/italics to identify what would be duplicate info to review&lt;/p&gt;</textHTML>
<x>15</x>
<y>8</y>
<commentparent>3950</commentparent>
</comment>
<comment><textHTML>&lt;p&gt;There will likely be a need for space for this in Class 1.&lt;br&gt;Suggestion: Include the video + Incentivize&lt;/p&gt;</textHTML>
<x>18</x>
<y>12</y>
<commentparent>3952</commentparent>
</comment>
<comment><textHTML>&lt;p&gt;Include links to readings?&lt;/p&gt;</textHTML>
<x>23</x>
<y>7</y>
<commentparent>3953</commentparent>
</comment>
<comment><textHTML>&lt;p&gt;Is there a grade here for the coding assignment and theoretical assignment separately?&lt;/p&gt;&lt;p&gt;Suggestion: add percentage (e.g. 2%) to indicate the portion of the total grade.&lt;/p&gt;</textHTML>
<x>11</x>
<y>10</y>
<commentparent>3956</commentparent>
</comment>
<comment><textHTML>&lt;p&gt;When do students select groups and project data sets?&lt;/p&gt;</textHTML>
<x>32</x>
<y>16</y>
<commentparent>5698</commentparent>
</comment>
<comment><textHTML>&lt;p&gt;Is this assignment a homework assignment or project iteration? Does completing this mean they've completed a portion of their project?&lt;/p&gt;&lt;p&gt;When do they receive feedback on these project assignments?&lt;/p&gt;</textHTML>
<x>17</x>
<y>5</y>
<commentparent>3957</commentparent>
</comment>
<comment><textHTML>&lt;p&gt;***double check content: the same as "6: Code Demo &amp; Practice"&lt;/p&gt;</textHTML>
<x>14</x>
<y>10</y>
<commentparent>153</commentparent>
</comment>
<comment><textHTML>&lt;p&gt;Consider arrows/links? Ex: For the theory and project assignments, are they related in such a way that one contributes to the work of the other?&lt;/p&gt;</textHTML>
<x>18</x>
<y>8</y>
<commentparent>150</commentparent>
</comment>
<comment><textHTML>&lt;p&gt;Check if sub-modules are available.&lt;/p&gt;</textHTML>
<x>16</x>
<y>9</y>
<commentparent>4342</commentparent>
</comment>
<comment><textHTML>&lt;p&gt;More information about the project assignments are needed in the following weeks.&lt;/p&gt;</textHTML>
<x>12</x>
<y>4</y>
<commentparent>4081</commentparent>
</comment>
<wflegendx>940</wflegendx>
<wflegendy>400</wflegendy>
<wfoutcomesorttype>week</wfoutcomesorttype>
</wfdata></workflow>
<tag><tagname>CCE Competencies</tagname>
<tagid>541</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>Manipulate data structures and implement algorithms</tagname>
<tagid>542</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>C - Install libraries and programming tools </tagname>
<tagid>543</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>C - Import data in programing environment</tagname>
<tagid>544</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>C - Design and implement simple algorithms </tagname>
<tagid>549</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>C - Apply existing libraries to data</tagname>
<tagid>550</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>C - Visualize data and models</tagname>
<tagid>551</tagid>
<iscollapsed>true</iscollapsed>
</tag>
</tag>
<tag><tagname>Use best practices for software development</tagname>
<tagid>545</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>C - Version code with Git</tagname>
<tagid>546</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>C - Push / pull code from GitHub</tagname>
<tagid>552</tagid>
<iscollapsed>true</iscollapsed>
</tag>
</tag>
<tag><tagname>Data analysis and modelling competencies</tagname>
<tagid>547</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>C - Formulate relevant questions about datasets</tagname>
<tagid>553</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>C - Select data modelling techniques to answer a question</tagname>
<tagid>554</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>C - Prepare data for analysis (feature extraction, missing values, train-test-validation sets)</tagname>
<tagid>555</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>C - Apply classification, regression, and clustering to existing data</tagname>
<tagid>556</tagid>
<iscollapsed>true</iscollapsed>
</tag>
</tag>
<tag><tagname>Interpersonal and Leadership competencies </tagname>
<tagid>548</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>C - Present critical information to team</tagname>
<tagid>557</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>C - Find resources to solve a technical problem</tagname>
<tagid>558</tagid>
<iscollapsed>true</iscollapsed>
</tag>
</tag>
</tag>
<tag><tagname>Course Objectives</tagname>
<tagid>559</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>Understand the broad classes of methods to address machine learning models and data science challenges</tagname>
<tagid>560</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>Understand machine learning workflow/pipeline design and integration as product</tagname>
<tagid>561</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>Understand basic theoretical insights behind ML models/algorithms with some minor math in order to have longer potential for deeper topics in ML</tagname>
<tagid>562</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>Understand how to link the main algorithms to coding and implement thesemethods in machine learning and data analysis, feature engineering and visualization</tagname>
<tagid>563</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>Apply these algorithms on realistic problems using existing software libraries, toolkits or ecosystems in machine learning community</tagname>
<tagid>564</tagid>
<iscollapsed>true</iscollapsed>
</tag>
</tag>
<tag><tagname>Korbit-Inspired Competencies</tagname>
<tagid>2454</tagid>
<iscollapsed>true</iscollapsed>
<tag><tagname>Probability Basics</tagname>
<tagid>2455</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>K - Normal Distribution</tagname>
<tagid>2456</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Joint and Marginal Probability Distributions</tagname>
<tagid>2457</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Conditional Probability</tagname>
<tagid>2459</tagid>
<iscollapsed>true</iscollapsed>
</tag>
</tag>
<tag><tagname>Data Preprocessing</tagname>
<tagid>2458</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>K - What is a Dataset?</tagname>
<tagid>2460</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Exploratory Data Analysis</tagname>
<tagid>2461</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Overview of Data Preprocessing</tagname>
<tagid>2462</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - One-Hot Encoding</tagname>
<tagid>2463</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Feature Importance</tagname>
<tagid>2464</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Feature Selection</tagname>
<tagid>2465</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Feature Scaling</tagname>
<tagid>2466</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Feature Engineering</tagname>
<tagid>2467</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Principal Component Analysis</tagname>
<tagid>2468</tagid>
<iscollapsed>true</iscollapsed>
</tag>
</tag>
<tag><tagname>Machine Learning Models</tagname>
<tagid>2470</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>K - Logistic Regression Basic</tagname>
<tagid>2472</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Sigmoid Function</tagname>
<tagid>2473</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Binary Classification</tagname>
<tagid>2474</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Binary Classification for Imbalanced Classes</tagname>
<tagid>2475</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Evaluation Metrics (Classification)</tagname>
<tagid>2476</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Linear Regression</tagname>
<tagid>2477</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Evaluation Metrics (Regression)</tagname>
<tagid>2478</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Clustering</tagname>
<tagid>2479</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Parameters vs. Hyperparameters</tagname>
<tagid>2480</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Hyperparameter Tuning</tagname>
<tagid>2481</tagid>
<iscollapsed>true</iscollapsed>
</tag>
</tag>
<tag><tagname>Machine Learning Overview</tagname>
<tagid>2482</tagid>
<iscollapsed>false</iscollapsed>
<tag><tagname>K - Motivation</tagname>
<tagid>2483</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Supervised Learning</tagname>
<tagid>2484</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Linear Approximators</tagname>
<tagid>2485</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Generalized Linear Approximators</tagname>
<tagid>2486</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Overfitting and Underfitting</tagname>
<tagid>2487</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Overview of Cross-Validation</tagname>
<tagid>2488</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Bias-Variance Decomposition</tagname>
<tagid>2489</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Overview of Logistic Regression</tagname>
<tagid>2490</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Gradient Descent</tagname>
<tagid>2491</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Regularization for Logistic Regression</tagname>
<tagid>2492</tagid>
<iscollapsed>true</iscollapsed>
</tag>
<tag><tagname>K - Lecture Summary: Introduction to Machine Learning</tagname>
<tagid>2493</tagid>
<iscollapsed>true</iscollapsed>
</tag>
</tag>
</tag>
</project>
